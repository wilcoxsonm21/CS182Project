inherit:
- models/small-lora.yaml
- wandb.yaml
model:
  n_dims: 1
  n_positions: 131
  pretrained_model_dir: /global/scratch/users/mortsven/wandb/models/kernel_linear_regression/big_base_nopos
  positional_encodings: False
  lora_config:
        r: 4 # rank
        lora_alpha: 16
        bias: none
        lora_dropout: 0.0
        target_modules: ["attn.c_attn"]
out_dir: /global/scratch/users/mortsven/wandb/models/loras
training:
  resume_id: lora12
  batch_size: 64
  curriculum:
    deg:
      end: 3
      inc: 0
      interval: 500001
      start: 3
    dims:
      end: 1
      inc: 1
      interval: 2000
      start: 1
    points:
      end: 31
      inc: 1
      interval: 1000
      start: 31
  data: uniform
  keep_every_steps: 100000
  learning_rate: 5.0e-05
  save_every_steps: 1000
  task: chebyshev_kernel_linear_regression
  task_kwargs:
    basis_dim: 5
    fixed_coeffs: 0
    highest_degree: 5
    lowest_degree: 5
    scale: 2.0
  train_steps: 2000000
wandb:
  name: lora12
