model:
    family: gpt2-lora
    n_embd: 128
    n_layer: 6
    n_head: 4