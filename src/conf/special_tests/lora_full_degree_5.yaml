inherit: 
    - models/small-lora.yaml
    - wandb.yaml

model:
    n_dims: 1
    n_positions: 41
    prompt_dim: 10
    pretrained_model_dir: ../models/kernel_linear_regression/small-1-11-5M-steps
    lora_config:
        r: 16 # rank
        lora_alpha: 16
        bias: none
        lora_dropout: 0.0
        target_modules: ["attn.c_attn", "mlp.c_fc", "mlp.c_proj"]

training:
    task: kernel_linear_regression
    data: gaussian
    task_kwargs: {"basis_dim": 5, "degree": 5}

    batch_size: 64
    learning_rate: 0.00005
    save_every_steps: 1000
    keep_every_steps: 100000
    train_steps: 300001
    curriculum:
        dims:
            start: 1
            end: 1
            inc: 1
            interval: 2000
        points:
            start: 31
            end: 31
            inc: 1
            interval: 1000
        deg: 
            start: 5
            end: 5
            inc: 0
            interval: 500001

out_dir: ../models/lora

wandb:
    name: "lora_full_degree_5"